# Projeto Delegacia 5.0: Microservi√ßo de IA (RAG)

Este reposit√≥rio cont√©m o **Microservi√ßo de IA** para o projeto "Delegacia 5.0 ‚Äì Chatbot Humanizado".

Este servi√ßo √© o "c√©rebro" do chatbot e √© respons√°vel por duas tarefas principais:
1.  **Classifica√ß√£o de Inten√ß√£o:** Entender o que o usu√°rio deseja fazer (ex: `registrar_bo`, `localizar_unidade`).
2.  **Gera√ß√£o de Resposta (RAG):** Buscar informa√ß√µes em uma base de conhecimento (documentos `.pdf`, `.txt`, `.docx`) para responder perguntas sobre dicas de seguran√ßa, localiza√ß√£o de delegacias, etc.

Este servi√ßo foi constru√≠do para ser consumido por um **Orquestrador de Chat** (ex: um servidor Node.js com WebSocket), desacoplando a l√≥gica de IA dos canais de comunica√ß√£o.

---

## 1. Instala√ß√£o e Configura√ß√£o (Passo a Passo)

Siga os passos abaixo para configurar e executar este servi√ßo localmente.

### Pr√©-requisitos

* [Git](https://git-scm.com/downloads)
* [Docker](https://www.docker.com/products/docker-desktop/) e **Docker Compose**
* [Python 3.11+](https://www.python.org/downloads/)
* Uma conta gratuita no **[Groq](https://groq.com/)** para obter uma API Key.

### Passo 1: Clonar o Reposit√≥rio

```bash
git clone [https://github.com/miguelamaral254/API-IA-RAG-DPC5.git](https://github.com/miguelamaral254/API-IA-RAG-DPC5.git)
````
````bash
cd API-IA-RAG-DPC5
````

### Passo 2: Configurar o Ambiente (`.env`)

Crie um arquivo chamado `.env` na raiz desta pasta (`API-IA-RAG-DPC5`). Copie e cole o conte√∫do abaixo, substituindo os valores `...` pelos seus.

**Arquivo `.env` (use como modelo para seu `.env`)**

```ini
# Configura√ß√µes do Banco de Dados (para scripts locais)
DATABASE_HOST=localhost
DATABASE_PORT=5433
DATABASE_USER=user
DATABASE_PASSWORD=password
DATABASE_NAME=delegacia_db

# Configura√ß√£o do PGVector
COLLECTION_NAME=delegacia_docs

# Configura√ß√£o do Ollama (Embeddings Locais)
# Usado para vetorizar os documentos (Etapa 1 do RAG)
OLLAMA_BASE_URL=http://localhost:11434
LLM_MODEL_NAME=phi3:mini

# Configura√ß√£o do Groq (Chat/Gera√ß√£o Online)
# Usado para "pensar" e gerar as respostas (Etapa 2 do RAG)
GROQ_API_KEY=gsk_SUA_CHAVE_API_DO_GROQ_AQUI
```

**Nota de Seguran√ßa:** Adicione `.env` ao seu arquivo `.gitignore` para nunca comitar suas chaves secretas\!

### Passo 3: Iniciar os Servi√ßos de Depend√™ncia (Docker)

O banco de dados e o servidor de *embeddings* (Ollama) rodam via Docker.

```bash
# 1. Inicie o banco (db) e o ollama (na pasta raiz do projeto)
docker-compose up -d db ollama

# 2. Baixe o modelo de embedding (phi3:mini) para dentro do container ollama
# (Isso s√≥ precisa ser feito uma vez)
docker-compose exec ollama ollama pull phi3:mini
```

### Passo 4: Configurar o Ambiente Python

Este servi√ßo roda localmente em um ambiente virtual (`venv`) para desenvolvimento.

```bash
# 1. Crie a venv (ambiente virtual)
python3 -m venv venv

# 2. Ative a venv
source venv/bin/activate

# 3. Instale todas as depend√™ncias do Python
pip install -r requirements.txt
```

-----

## 2\. Como Usar

Com o ambiente configurado, siga estes passos para rodar a API.

### Passo 5: Alimentar o RAG (Ingest√£o de Dados)

O RAG precisa "aprender" com seus documentos.

1.  **Adicione seus arquivos (`.pdf`, `.docx`, `.txt`) na pasta `/data`.**
2.  Execute o script de ingest√£o (com a `venv` ativa):

<!-- end list -->

```bash
# O script vai ler a pasta /data, vetorizar os arquivos (usando o Ollama)
# e salvar tudo no banco de dados pgvector.
python scripts/ingest.py
```

**Sa√≠da Esperada:** `Ingest√£o conclu√≠da!`

### Passo 6: Executar a API

Com as depend√™ncias rodando (Docker) e o banco alimentado (Ingest√£o), inicie o servidor FastAPI:

```bash
# (Com a venv ativa)
uvicorn app.main:app --reload --port 8001
```

O servidor estar√° dispon√≠vel em `http://127.0.0.1:8001`.

-----

## 3\. Arquitetura e Stack Tecnol√≥gica

### Diagrama da Arquitetura Completa

O diagrama abaixo ilustra o fluxo de dados desde o usu√°rio (front-end) at√© os back-ends de processamento e integra√ß√£o:

````mermaid
graph TD
    subgraph "Usu√°rio"
        F["üåê Frontend (React)"]
    end

    subgraph "Servidor de Chat (Orquestrador)"
        WS["üöÄ Orquestrador (Node.js + WebSocket)"]
    end

    subgraph "Microsservi√ßos de Backend"
        NLP["üß† API de IA - RAG <br/> Python/FastAPI <br/> <b>[Este Reposit√≥rio]</b>"]
        BO_API["üìã API de A√ß√µes - BO <br/> Node.js/Express"]
    end

    subgraph "Bancos de Dados (Docker)"
        DB["üêò PostgreSQL"]
        PGVEC["Vector<br/>pgvector"]
    end

    subgraph "Servi√ßos Externos"
        GROQ["‚ö°Ô∏è Groq API (LLM R√°pido)"]
        OLLAMA["ü§ñ Ollama (Embeddings)"]
    end

    %% -- Fluxo de Chat (RAG) --
    F <-->|"1. Envia Msg (Socket)"| WS
    WS -->|"2. Classifica Intaen√ß√£o"| NLP
    NLP -->|"3. Chama LLM (Gera√ß√£o)"| GROQ
    GROQ -->|"4. Retorna Inten√ß√£o"| NLP
    NLP -->|"5. Retorna Inten√ß√£o"| WS
    WS -->|"6. Pede Resposta (RAG)"| NLP
    NLP -->|"7. Vetoriza Pergunta"| OLLAMA
    NLP -->|"8. Busca no Vetor"| PGVEC
    PGVEC -->|"9. Retorna Contexto"| NLP
    NLP -->|"10. Gera Resposta (LLM)"| GROQ
    GROQ -->|"11. Retorna Resposta"| NLP
    NLP -->|"12. Retorna Resposta"| WS
    WS -->|"13. Envia Resposta (Socket)"| F

    %% -- Fluxo de A√ß√£o (BO) --
    WS -->|"Comando: Abrir Modal"| F
    F -->|"Envia Dados (Form via REST/POST)"| BO_API
    BO_API -->|"Salva no Banco"| DB
````

### Stack Tecnol√≥gica (Este Servi√ßo: API-IA-RAG)

  * **Linguagem:** Python 3.11+
  * **Framework:** FastAPI
  * **Orquestra√ß√£o de IA:** LangChain
  * **LLM (Gera√ß√£o/Classifica√ß√£o):** Groq API (ex: `llama-3.1-70b-versatile`) - R√°pido, via API.
  * **Embeddings (Vetoriza√ß√£o):** Ollama (ex: `phi3:mini`) - Gratuito, rodando localmente via Docker.
  * **Banco de Dados:** PostgreSQL com a extens√£o `pgvector`.
  * **Infraestrutura:** Docker e Docker Compose.

-----

## 4\. Integra√ß√£o de Canal (Web Chat)

Conforme os requisitos do Projeto Integrador, a integra√ß√£o com o canal de chat (um web chat embutido) **√© desacoplada** deste servi√ßo de IA.

A arquitetura foi projetada para que um **Servi√ßo Orquestrador** (o `chat_orchestrator` em Node.js no diagrama) seja o respons√°vel por gerenciar a comunica√ß√£o com o cliente (seja ele um site, um app, ou o WhatsApp).

### Exemplo de Fluxo (Desacoplado)

1.  **Canal Web (Web Chat Embutido):**

      * Um frontend React se conecta ao Orquestrador via **WebSocket (Socket.io)**.
      * Quando o usu√°rio envia uma mensagem, o Orquestrador a recebe.
      * O Orquestrador ent√£o chama esta API (`/classify` ou `/query/rag`) para obter uma resposta.
      * A resposta da API √© enviada de volta ao React pelo WebSocket.

2.  **Exemplo de Canal Alternativo (Ex: WhatsApp):**

      * Um usu√°rio envia uma mensagem para o n√∫mero do WhatsApp.
      * A API do WhatsApp envia um **Webhook** para o Orquestrador.
      * O Orquestrador recebe o Webhook, trata a mensagem e chama esta API (`/classify` ou `/query/rag`).
      * A resposta da API √© formatada e enviada de volta para a API do WhatsApp, que a entrega ao usu√°rio.

Em ambos os cen√°rios, esta **API de IA n√£o muda**. Ela √© agn√≥stica ao canal, garantindo modularidade e escalabilidade.

-----

## 5\. Endpoints da API (Testando com Postman)

### Classificar Inten√ß√£o

  * **URL:** `http://localhost:8001/classify`
  * **M√©todo:** `POST`
  * **Body (JSON):**
    ```json
    {
        "query": "Como eu registro um furto de celular?"
    }
    ```
  * **Resposta (200 OK):**
    ```json
    {
        "intent": "registrar_bo"
    }
    ```

### Responder Pergunta (RAG)

  * **URL:** `http://localhost:8001/query/rag`
  * **M√©todo:** `POST`
  * **Body (JSON):**
    ```json
    {
        "query": "Onde fica a delegacia da mulher no Recife?"
    }
    ```
  * **Resposta (200 OK):**
    ```json
    {
        "answer": "A 1¬™ Delegacia da Mulher - Recife fica na Rua XYZ, 987, Santo Amaro, Recife-PE. O telefone √© (81) 9999-8888 e sua especialidade √© o atendimento a v√≠timas de viol√™ncia dom√©stica e familiar."
    }
    ```
